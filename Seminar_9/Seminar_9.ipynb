{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](http://www.nltk.org/book/) - пакет библиотек для анализа естественного языка\n",
    "\n",
    "Информация для [установки NLTK](http://www.nltk.org/install.html)\n",
    "\n",
    "Фоспользуемся функциональностью данного пакета для [выделения токенов](http://www.nltk.org/_modules/nltk/tokenize.html). Для этого из  нужно импортировать word_tokenize из nltk.tokenize. word_tokenize - это функция, которая принемает на вход текст, а возвращает список токенов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задача 1. ** Прочитайте тексты статей из articles.csv (что бы ускорить дольнейшие эксперименты возьмите первые 500). Преобразуйте каждую статью в список токенов. Получите список списков токенов. Сколько уникальных токенов содержится во всех текстах (не забудьте привести тексты к одному регистру)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) позволяет находить основу слова\n",
    "\n",
    "Отметим, что основа слова не обязательно совпадает с морфологическим корнем слова\n",
    "\n",
    "Для выделения основы слова можно использовать [Стеммер Портера](https://tartarus.org/martin/PorterStemmer/). Из nltk.stem.porter импортируйте PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее создаём стеммер (экземпляр класса PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У класса PorterStemmer есть метод stem, который принемает на вход слово, а возвращает его нормализованную форму"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 2. ** Преобразуте токены из текстов с помошью стемера портера. Какое время понадобится, что бы нормализовать все тексты из articles.csv. Сколько уникальных токенов содержится во всех текстах после стеминга?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) позваляет привести словоформу к нормальной форме\n",
    "\n",
    "Для того чтобы лемматизировать текст из nltk.stem нужно импортировать WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее создаём лемматизатор (экземпляр класса WordNetLemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У класса WordNetLemmatizer есть метод lemmatize, который принемает на вход слово, а возвращает его нормализованную форму"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.**  Лемматизируйте токены из текстов. Какое время понадобится, что бы нормализовать все тексты из articles.csv. Сколько уникальных токенов содержится во всех текстах теперь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working -> working\n",
      "working -> work\n",
      "works -> work\n"
     ]
    }
   ],
   "source": [
    "word = 'working'\n",
    "lemma_word = lemmatizer.lemmatize(word)\n",
    "print('%s -> %s'%(word, lemma_word))\n",
    "\n",
    "lemma_word = lemmatizer.lemmatize(word, pos='v')\n",
    "print('%s -> %s'%(word, lemma_word))\n",
    "\n",
    "word = 'works'\n",
    "lemma_word = lemmatizer.lemmatize(word)\n",
    "print('%s -> %s'%(word, lemma_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнать часть речи слова можно с помощью метода [nltk.pos_tag](http://www.nltk.org/book/ch05.html)\n",
    "\n",
    "Заметим, что параметр pos в методе lemmatize может принемать только 4 значения (соответствующие глаголу, существительному, прилагательному и наречию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.** Напишите функцию, которая на вход принемает список токенов, а возвращает нормализованный список токенов. Нужно учитовать часть речи. Какое время понадобится, что бы нормализовать все тексты из articles.csv. Сколько уникальных токенов содержится во всех текстах теперь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
