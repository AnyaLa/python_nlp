{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python. Сrawling\n",
    "\n",
    "Данный семинар посвящен выкачиванию страниц с помощью Python\n",
    "\n",
    "Для скачивания HTML-страниц в питоне есть специальный модуль [urllib.request](https://docs.python.org/3.0/library/urllib.request.html)\n",
    "\n",
    "**Пример. ** Скачаем главную страницу https://habrahabr.ru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "\n",
    "req = urllib.request.Request('https://habrahabr.ru/')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    html = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что мы скачали"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / Хабрахабр</title>\n"
     ]
    }
   ],
   "source": [
    "print(html[:228])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачали html-код страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = urllib.request.Request('https://ru.wikipedia.org')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    html = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда сайт блокирует запросы, если их посылает не настоящий браузер с пользователем, а какой-то бот. Иногда сайты присылают разные версии страниц, разным браузерам.\n",
    "\n",
    "По этим причинам полезно бывает писать скрипт, который умеет притворяться то одним, то другим браузером. Когда мы пытаемся получить страницу с помощью urllib, наш код по умолчанию честно сообщает серверу, что он является программой на питоне, но можно, например, представиться Мозиллой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://habrahabr.ru/'\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "\n",
    "req = urllib.request.Request('https://habrahabr.ru/', \n",
    "                             headers={'User-Agent':user_agent})  \n",
    "\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    html = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили код страницы, теперь нам нужно вытащить из неё информацию, например заголовки статей \n",
    "\n",
    "Для начала нужно посмотреть в исходник и заметить, что заголовки хранятся в тэге h2 с классом post__title\n",
    "\n",
    "Далее следует воспользоваться регулярными выражениями и модулям [re](https://docs.python.org/2/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество заголовков на данной странице 10\n",
      "Посмотрим на первый заголовок:\n",
      "<h2 class=\"post__title\">\n",
      "    <a href=\"https://habrahabr.ru/post/344616/\" class=\"post__title_link\">Критика 1С</a>\n",
      "  </h2>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regPostTitle = re.compile('<h2 class=\"post__title\">.*?</h2>', \n",
    "                          re.DOTALL)\n",
    "titles = regPostTitle.findall(html)\n",
    "\n",
    "print('Количество заголовков на данной странице %d'%len(titles))\n",
    "print('Посмотрим на первый заголовок:')\n",
    "print(titles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нужно отчистить заголовки от лишних тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles_cleaner(title):\n",
    "    title = re.sub(r'<[^<>]*>', '', title)\n",
    "    title = re.sub(r'\\n', '', title)\n",
    "    return title.strip()\n",
    "\n",
    "titles_clean = [titles_cleaner(title) for title in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, что у нас получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Критика 1С\n",
      "1. Рожденный перехватывать трафик\n",
      "2. Vivaldi Sync — первое знакомство\n",
      "3. Интернет вещей: Arduino в связке с облаком\n",
      "4. Когда биткоин перестанет расти: токены — настоящая альтернатива коинам\n",
      "5. Интеллектуальные чат-боты на ChatScript: основы\n",
      "6. Разработка через приемочные тесты (ATDD). Что это такое, и с чем его едят\n",
      "7. Битва за сетевой нейтралитет: история вопроса\n",
      "8. Итальянская забастовка роботов\n",
      "9. Фэйковый дизайн\n"
     ]
    }
   ],
   "source": [
    "for i, title in enumerate(titles_clean):\n",
    "    print('%d. %s'%(i, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 1. ** Скачать главную страницу Яндекс.Погоды и сделать следующее: (1) распечатать сегодняшнюю температуру и облачность, (2) распечатать время восхода и заката, (3) погоду на завтра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 2.** Скачать главную страницу waitbutwhy.com. Распечатать заголовки популярных постов (которые в колонке справа с надписью Popular Posts) и колличество комментариев у каждого из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы научимся выкачивать содержимое не одной страницы, а целого сайта или даже многих сайтов. Понятно, что перечислить все интересующие нас адреса вручную, чтобы выкачать их по отдельности, в таком случае не получится.\n",
    "\n",
    "Основной проблемой сотоит в том, как узнать адреса всех страниц, которые требуется скачать. Рассмотрим два подхода:\n",
    "\n",
    "1. *Первый подход* обычно применяется, когда нужно загрузить все страницы какого-нибудь крупного ресурса --- например, газеты или форума. Адреса страниц на таких сайтах нередко устроены довольно просто: начинаются они все одинаково, а заканчиваются разными числами. Если внимательно посмотреть на адреса нескольких произвольных страниц, можно довольно быстро выяснить, так ли это и каков допустимый диапазон номеров страниц. В этом случае закачка всех страниц будет представлять собой простой цикл, в котором будут перебираться все номера страниц из этого диапазона.\n",
    "\n",
    "2. *Второй подход* обычно применяется в краулерах --- программах, которые обходят какой-то фрагмент интернета, собирая информацию с разных сайтов. Краулерами, например, пользуются поисковые системы, чтобы индексировать содержимое сайтов. Краулер начинает работу с одной или нескольких страниц, адреса которых задаются вручную, а затем переходит по всем ссылкам из этих страниц. Каждый раз, когда краулер загружает очередную страницу, он находит на ней не только нужную ему информацию, но и все ссылки, которые добавляются в очередь. Важно при этом помнить, где краулер уже побывал, чтобы не переходить по нескольку раз на одни и те же страницы. В настоящих краулерах применяют и другие ухищрения, например, чтобы выяснить, по каким ссылкам лучше переходить сначала, но мы этого касаться не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример. ** Допустим хотим скачать все посты главного албанского форума http://www.forumishqiptar.com\n",
    "\n",
    "С ходу трудно разобраться, что там к чему, но если присмотреться, станет понятно, что перед нами список тем. Нажав на какую-нибудь тему, мы попадём на страницу со списком тредов, а нажав на тред, обнаружим страницу с постами. Это, видимо, и есть та страница нижнего уровня, которые мы хотим скачивать.\n",
    "\n",
    "Посмотрим на адреса нескольких таких страниц. Выглядят они примерно так:\n",
    "\n",
    "http://www.forumishqiptar.com/threads/79403-%C3%87far%C3%AB-%C3%ABsht%C3%AB-dashuria\n",
    "\n",
    "http://www.forumishqiptar.com/threads/41551-P%C3%ABrkufizimi-i-dashuris%C3%AB%21%21\n",
    "\n",
    "http://www.forumishqiptar.com/threads/160424-Mos-luaj-me-dashurin\n",
    "\n",
    "Обратим внимание, на то что начало у всех ссылок одинаковое, а далее идёт номер. Если бы дело ограничелось только номером, то их можно было бы с лёгкостью перебрать их все, но мы видим, что после номера идёт ещё и текст, а вот это уже проблема. Одним из решений было бы загрузить сначала страницы с темами и найти в них полные ссылки на треды. \n",
    "\n",
    "Однако мы применим хитрость, которая срабатывает в 90% случаев вроде этого. Попробуем взять адрес какой-нибудь страницы, вручную убрать в нём всё после числа и ввести это в адресную строку браузера. \n",
    "Вуаля! Всё работает и без заголовка (точнее, происходит автоматическое перенаправление). Это значит, что мы можем пользоваться перебором номеров. Для этого нам достаточно узнать диапазон -- номера самой первой и самой последней (по времени) страницы -- это остаётся в качестве **упражнения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c84d425e7646f39c400d6e0be4e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160400: kalohet, artikulli, http://zeri.info/aktuale/19017/40-kosovare-te-ngujuar-ne-meksike-kerkojne-ndihmen-e-qeverise-kosoves, kushton, artikull, meksika, http://www.reporter.al/nga-bjeshket-ne-bronks-udhetimi-ilegal-i-shqiptareve-drejt-amerikes, veshtiresite\n",
      "\n",
      "160401: perdoret, amerike, legalisht, kuptohet, kalimin, meksike, shqipetar, perkohsisht, pershkak\n",
      "\n",
      "160402: meksike, shqipetar, shqiptare\n",
      "\n",
      "160403: shqiptari, https://www.youtube.com/watch?v=j_y4gljvga0, bubrrec, kelmendi\n",
      "\n",
      "160404: dashuria, dashuri, dashurinë, ndoshta, njerëzit, keqardhje, vetvetes, instikt, padobishëm, jetojmë, këshillë, mallkuar, shekullin, kërkesa, ekzistonte, konceptohet, dëshira, rrënjët, rastësia, vërtetë, dëbohesh, problemin, instikti, ndihesha, ndryshe, nevojën, kurrësesi, arsyeshëm, shqetësohesha, partnere, instiktivisht, sugjeroj, jetojnë, shqetëson, kundërta, saktësi, kërkosh\n",
      "\n",
      "160405: erdoganit, vellazerise, harroni, mbeshtesin, mbeshtesni, erdoganin, padrejtesia, myslimane, njerezve, trajtim, postimin, shqiptaret, rrallehere, drejtesia, shpeshhere, arrestimin, mehershem, zgjedhjet, demokratike, fitonte, milloshevici, putinin, diktatore, abuzues, pushtet, stupcin, granatat, kallashnikovin, https://www.youtube.com/watch?v=d62ny0xdprm, zbatonte, nenkupton, demokracia, organizohej, krijuesit, llogari, memorie, milloshevicit, zoterinje, atehere, luftoje, arriten, rrezonin, elikopterin, luftarak, kundeshtareve, arrestuan, marrezira, seriozisht.lexova, organizuar.fillon, keshilltare\n",
      "\n",
      "160406: krasniqi, doberstein, sidomos, gjyqtarëve, boksierit, arritje, përshëndet, nokautonte, kundërshtarin, titullin, tifozët, arritur, http://www.gazetaexpress.com/sport/krasniqi-i-pershendet-tifozet-ne-gjuhen-shqipe-227376/?utm_source=website&amp;utm_medium=ra&amp;utm_content=haxhi%20krasniqi,boks&amp;utm_campaign=insidearticle&amp;past_aid=, /express, krasniqit, intercontinental, shqiptar, ngjitet, ndeshja, boksierin, kosovar, ndeshje, boksieri, shkëlqyer, 11&amp;8217të, raundin, vendimit, gjerman, dominuar, dominoi\n",
      "\n",
      "160407: edi rama, ilir meta, reforma ne drejtesi, tiranes, ndershem, paskemi, trim-bujar, tvertet, drejtësi, rrëzojë, reforma, qeverinë, monument, reformen, parakohshme=harrojeni, zgjedhje\n",
      "\n",
      "160408: turqi, erdoganit, shqiptaret, politike, shqiptar, grupeve, terrorizmit, familjen, shkojne, gjendje, karieren, shqiptare, terrorizoni, qafiret, shkelni, kamjone, mundeni, autobuse, hudheni, sponzorimit, myslimane, triumfoje, prenjani, allahut, maqedoni, lindore, neveritshme, vertete, mjerueshme, shqiperi, ndrequr, shiptaret, banuara, pushtet, ngjallja, shpirtit, qamilit, shkarkuar, grushtin, erdogan, shumice, mallkuar, partise, diktatoret, drejtesine, shtetit, erdogani\n",
      "\n",
      "160409: pershndetje, qellimi, gjelboje, shartoje, zgukuroje\n",
      "\n",
      "160410: ëndërra, ëndërrat, deklaroj, retorika, përgjithësisht, mendimi, deshirat, enderrat, dëshira, pershendetnit, deshire, vecanërisht, specifikisht, përshëndes, menduar, vetmashtrim, thjeshtë, optimist, vizitoni, ngadalësoj, martuar, divorcuar, shoqerore, gjendja, shkolle, mbaruar, psikologji, pershendetje, rastësia, interesash, përpjekje, dështim, kthehet, profesioni, specialisti, gjithmonë, psikolog, shpresoj\n",
      "\n",
      "160411: përshndetje, qellimi, vështir, tndryshohet\n",
      "\n",
      "160412: drejtit, ndryshe, rrespekt, qellimi, mbrenda, idenave, individi, përshndetje, personale, familja, shtëpija, gjithve, prishim\n",
      "\n",
      "160413: tevitohet, përshndetje, qellimi, oksidente\n",
      "\n",
      "160414: forumi, forumi shqiptar, shqiperia, albania, albasoul, shqiptar, albanian, shqip, shqiperi, shqiperia, shqiptar, kosova, kosovo, muzike, letersi, art, humor, #shqiperia, diskuto, kerko, shkolla, artikull, filma, kenga, muzika, videoja, fotoja\n",
      "\n",
      "160415: përshndetje, rrespekt, qellimi, drejtësin, ndryshosh, nambëlcinë, tsheqerit\n",
      "\n",
      "160416: heshtur, gjithçka, perseri, imazhin, pentagrame, https://www.youtube.com/watch?v=wrnbesaac-4, refreni, festivali, http://www.teksteshqip.com/ermira-babaliu/lyric-1707885.php, ndoshta, babaliu, enderrova, perqafoj, vargjet, dangellia, kompozimi, emocion\n",
      "\n",
      "160417: ateiste, besimit, shqiptareve, shqiptaret, ortodokse, myslimane, njejtit, gyleniste, holandezit, francezit, danezit, suedezit, interesant, kombesine, krishterimi, gjermanit, organizatat, perparuara, krishterimin, akuzojne, krishtere, përpara, kunderta, kombesi, identitet, genjeshtare, interes, vrasjen, gjendje, interesin, kombetar, drejtesine, lepijne, shembullin, merreni, njeriut, partner, shtrigave, shpirtin, jezusit, strukturave, ortodoks, anti-erdogan, politikave, gylenit, analistët, opinion, shqipëri, sqaruar, politika\n",
      "\n",
      "160418: https://www.youtube.com/watch?v=oeo0ders8e8, trileçe\n",
      "\n",
      "160419: forumi, forumi shqiptar, shqiperia, albania, albasoul, shqiptar, albanian, shqip, shqiperi, shqiperia, shqiptar, kosova, kosovo, muzike, letersi, art, humor, #shqiperia, diskuto, kerko, shkolla, artikull, filma, kenga, muzika, videoja, fotoja\n",
      "\n",
      "160420: forumi, forumi shqiptar, shqiperia, albania, albasoul, shqiptar, albanian, shqip, shqiperi, shqiperia, shqiptar, kosova, kosovo, muzike, letersi, art, humor, #shqiperia, diskuto, kerko, shkolla, artikull, filma, kenga, muzika, videoja, fotoja\n",
      "\n",
      "160421: forumi, forumi shqiptar, shqiperia, albania, albasoul, shqiptar, albanian, shqip, shqiperi, shqiperia, shqiptar, kosova, kosovo, muzike, letersi, art, humor, #shqiperia, diskuto, kerko, shkolla, artikull, filma, kenga, muzika, videoja, fotoja\n",
      "\n",
      "160422: forumi, forumi shqiptar, shqiperia, albania, albasoul, shqiptar, albanian, shqip, shqiperi, shqiperia, shqiptar, kosova, kosovo, muzike, letersi, art, humor, #shqiperia, diskuto, kerko, shkolla, artikull, filma, kenga, muzika, videoja, fotoja\n",
      "\n",
      "160423: tnderuem, pershndetje, qellimi, dashuri\n",
      "\n",
      "160424: qellimi, dashurin, rrespekt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def download_page(pageUrl):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(pageUrl)\n",
    "        html = page.read().decode('ISO-8859-1')\n",
    "        keywords = re.findall('<meta name=\"keywords\" .* />', html)[0]\n",
    "        keywords = keywords.replace('<meta name=\"keywords\" content=\"'\n",
    "                                    , '')\n",
    "        keywords = keywords.replace('\" />', '')\n",
    "        return keywords\n",
    "    except urllib.request.HTTPError:\n",
    "        return 'Error at %s'%pageUrl\n",
    "\n",
    "commonUrl = 'http://www.forumishqiptar.com/threads/'\n",
    "for i in tqdm(range(160400, 160425)):\n",
    "    pageUrl = commonUrl + str(i)\n",
    "    print('%d: %s'%(i, download_page(pageUrl)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание. ** Когда Ваша программа выкачивает много страниц сразу, она создаёт нагрузку на сервер выкачиваемого сайта -- и эта нагрузка намного больше, чем нагрузка от обычного посещения сайта людьми. Если Вы выкачиваете содержимое крупного сайта с одного компьютера, то ничего страшного в этом, скорее всего, нет. Но если это не какой-то крупный ресурс, который владеет мощными серверами, и тем более если страницы с него скачивают несколько человек одновременно, это может создать реальные проблемы владельцам выкачиваемого ресурса. Поэтому, во-первых, нужно всегда выяснять, не доступно ли всё содержимое нужного Вам ресурса по отдельной ссылке (например, так обстоит дело с Википедией), а во-вторых, ставить между обращениями к серверу искуственный временной интервал хотя бы в пару секунд:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание.** Нужно не забывать, что для отображения на html-странице символов, которых нет на клавиатуре, применяются специальные последовательности символов, начинающиеся с амперсанда (&) и заканчивающиеся точкой с запятой (;). Чтобы получить текст не с такими последовательностями, а с нормальными символами, используется специальная функция в питоне unescape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Петя & Вася\n",
      "Специальные символы: \" « » ♠ ♥ ♣ ♦ и так далее\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "print( html.unescape('Петя &amp; Вася'))\n",
    "print( html.unescape('Специальные символы: &quot; &laquo; &raquo; &spades; &hearts; &clubs; &diams; и так далее'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 3.** Вообще-то в длинных постах бывает по многу страниц, например, как тут: http://www.forumishqiptar.com/threads/79403 Нужно написать код, который умеет скачивать все страницы треда, а не только первую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
