{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam vs. Ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целью данного задания является научится отличать письма со спамом "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 1 (2 балла). Чтение данных ** \n",
    "\n",
    "Прочитайте данные из файла spam.csv используя средства модуля [pandas](https://pandas.pydata.org/). В первом столбце пометка о роде письма (spam/ham). Будем считать, что все письма с пометкой spam лежать в первом классе, а остальные в нулевом. Во второй колонке текст сообщения. Отобразите таблицу в следующем виде\n",
    "![example](table_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Задача 2 (3 балла). Предобработка ** \n",
    "\n",
    "Перевидите все буквы в нижний регистр. \n",
    "\n",
    "С помошью [регулярных выражений](https://docs.python.org/2/library/re.html) почистите текст отставляя только слова (удалите знаки припенания, а также все числа можно заменить на N и т.п.). \n",
    "\n",
    "Оформите это в виде функции, которая принимает на вход некоторый список текстов и возвращает, соответственно, предобработанный список текстов. \n",
    "\n",
    "Преобразуйте таблицу с данными, так чтобы в ней содержались предобработанные тексты (далее мы будем работать только с ними)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 3 (3 балла). Формирование словаря** \n",
    "\n",
    "Сформируйте словарь, ключами которого являются слова встречающееся в тексте, а значениями -- колличество раз, которое они встретились в корпусе текста. \n",
    "\n",
    "Оформите это в виде функции, которая принимает на вход список текстов и возвращает словарь. \n",
    "\n",
    "Какой размер словаря вы получили?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 4 (2 балла). Удаление стоп-слов** \n",
    "\n",
    "Отсортируйте слова по убыванию частоты их встречаемости в текстах. \n",
    "\n",
    "С помощью модуля [matplotlib.pyplot](https://matplotlib.org/api/pyplot_api.html) постройте график частот встречаемости слов в тексте. \n",
    "\n",
    "Распечатайте топ-10 слов и их частоты. \n",
    "\n",
    "Какие слова чаще всего встречаются в тексте? Значимые ли это слова? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 5 (5 баллов). Функциия предобработки и удаление стоп-слов. ** \n",
    "\n",
    "Для удаления стоп-слов можно использовать модуль [nltk](http://www.nltk.org/). Импортируйте stopwords из nltk.corpus, далее чтобы получить список английских стоп-слов нужно сделать stopwords.words('english').\n",
    "\n",
    "Перепишите функцию предобрабитки так, чтобы она на вход принимала список текстов и список стоп-слов, чистела текст (в том числе удаляла стоп-слова), возвращала список предобработанных текстов.  \n",
    "\n",
    "Преобразуйте таблицу с данными, так чтобы в ней содержались новые предобработанные тексты (далее мы будем работать только с ними). \n",
    "\n",
    "Сформируйте словарь. \n",
    "\n",
    "Постройте график частот встречаемости слов в тексте. \n",
    "\n",
    "Распечатайте топ-10 слов и их частоты. \n",
    "\n",
    "Какой размер словаря вы получили теперь? Остались ли ещё высокочастотные неинформативные слова? Что это за слова? Добавте их в список стоп-слов и повторите процедуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 6 (5 баллов). Удаление низко частотных слов** \n",
    "\n",
    "По сформированому на предыдущем шаге словарю, посмотрите какой процент слов встречается больше одного раза. Нужны ли нам слова которые встретились только один раз? \n",
    "\n",
    "Преобразуйте функцию предобработки таким образом, что бы в ней, кроме всего прочего, формировался словарь и удалялись слишком редкие слова. \n",
    "\n",
    "Преобразуйте таблицу с данными, так чтобы в ней содержались новые предобработанные тексты (далее мы будем работать только с ними). \n",
    "\n",
    "Сформируйте словарь. Каков размер словаря теперь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 7 (5 баллов). Представление в виде мешка слов** \n",
    "\n",
    "Реализуйте свою функцию, которая по списку текстов строит представление в виде мешка слов. На вход подаётся список текстов, а возвращает список имен признаков (т.е. слова по которым мы строим представление -- их порядок должен быть зафексирован) и numpy.array, который содержит вектора, каждая позиция которого -- число вхождений данного признака (слова) в текст.\n",
    "\n",
    "Сейчас мы реализовываем данную функцию в учебных целях, а вообще велосипед изобретать не стоит! Если вам нужно такое представление, то задействуйте модуль [sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text). Импортируйте из него класс [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Создайте экземпляр класса с параметрами по умолчанию, а далее примените метод fit_transform для извлечения векторного представления текста. Чтобы получить список имён призков нужно воспользоваться методом get_feature_names. Проверте что ваша функция работает также (с точностью до перестановки признаков). \n",
    "\n",
    "Измерте скорость работы вашей и метода fit_transform. Для этого можно воспользоваться функцией time из модуля time. Постарайтесь максимально оптимизировать вашу функцию (время работы функции не должно привышать 10 с., за привышения лимита будут сняты баллы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_vectorizer(texts):\n",
    "    #место для вашего кода\n",
    "    return features_names, count_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 8 (5 баллов). Представление с использованием TfIdf** \n",
    "\n",
    "Задание аналогичное заданию 7, но теперь  признаки должны содержать значение tf-idf. \n",
    "\n",
    "Реализация должна быть на numpy.array, а не на списках!\n",
    "\n",
    "Аналогичная функция также содержится в [sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(texts):\n",
    "    #место для вашего кода\n",
    "    return features_names, count_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задача 9 (10 баллов). Решение задачи классификации**\n",
    "\n",
    "Теперь с помощью наших представлений и [метода K ближайших соседей (KNN)](http://www.machinelearning.ru/wiki/index.php?title=KNN) ([нужный класс](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), [полезная ссылочка](https://habrahabr.ru/post/149693/)) научимся вычислять спам.\n",
    "\n",
    "Пусть X -- вектора признаков (полученные с помощью CountVectorizer или TfidfVectorizer), y -- вектор ответов (в нашем случае колонка class в таблице). \n",
    "\n",
    "Разделите (X, y) на обучающую выворку (X_train, y_train) (70%) и на тестовую -- (X_test, y_test) (30%). Разделить выборку можно вручную, но лучше воспользоваться [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "\n",
    "Далее обучите классификатор на (X_train, y_train) -- это делается с помощью метода fit. Затем получите y_pred (результат классификации) на тесте X_test с помощью метода predict. После чего сравните получившийся результат y_pred и y_test (правельные метки классов) с помощью функции [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). \n",
    "\n",
    "Поиграйте с параметром n_neighbors у KNN -- добейтесь наилучшего результата классификации.\n",
    "\n",
    "Сравните качество классификации для представлений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
